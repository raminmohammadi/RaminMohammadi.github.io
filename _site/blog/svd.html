<!doctype html>

<html class="no-js" lang="en">

<head>


	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

	ML with Ramin

	Personal Theme by https://jekyllthemes.io
	Premium + free Jekyll themes for your blog or website.

	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->


	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

	<!-- Page Info -->
	<link rel="shortcut icon" href="/images/favicon.png">
	<title>Singular Value Decomposition (SVD) ‚Äì ML with Ramin</title>
	<meta name="description" content="">

	<!-- Twitter Card -->
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="Singular Value Decomposition (SVD) ‚Äì ML with Ramin">
	<meta name="twitter:description" content="">
	<meta name="twitter:image:src" content="http://localhost:4000/images/demo/demo-square.webp">

	<!-- Facebook OpenGraph -->
	<meta property="og:title" content="Singular Value Decomposition (SVD) ‚Äì ML with Ramin" />
	<meta property="og:description" content="" />
	<meta property="og:image" content="http://localhost:4000/images/demo/demo-square.webp" />

	
	<!-- Font Embed Code -->
	<link href="https://fonts.googleapis.com/css?family=Muli:300,400,600,700" rel="stylesheet">
	

	<!-- Styles -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="/css/style.css">
	
	<!-- Icons -->
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/solid.js" integrity="sha384-GXi56ipjsBwAe6v5X4xSrVNXGOmpdJYZEEh/0/GqJ3JTHsfDsF8v0YQvZCJYAiGu" crossorigin="anonymous"></script>
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/brands.js" integrity="sha384-0inRy4HkP0hJ038ZyfQ4vLl+F4POKbqnaUB6ewmU4dWP0ki8Q27A0VFiVRIpscvL" crossorigin="anonymous"></script>
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/fontawesome.js" integrity="sha384-NY6PHjYLP2f+gL3uaVfqUZImmw71ArL9+Roi9o+I4+RBqArA2CfW1sJ1wkABFfPe" crossorigin="anonymous"></script>

	
	<!-- Custom Styles -->
	<style></style>
	

	
	<!-- Analytics Code -->
	
	

	
	<!-- Extra Header JS Code -->
	
	
	
</head>


<body class="loading ajax-loading" data-site-url="http://localhost:4000" data-page-url="/blog/svd">


	<header class="header">

	<div class="wrap">

		
		<a href="/" class="header__title">
			ML with Ramin
		</a>
		

		<div class="menu">
			<div class="menu__toggle js-menu-toggle">
				<div class="menu__toggle__icon"><span></span></div>
			</div>
			<div class="menu__wrap">
				<ul class="menu__list">
					
					<li class="menu__list__item">
						<a href="/about/" class="menu__list__item__link">About us</a>
					</li>
					
					<li class="menu__list__item">
						<a href="/ml/" class="menu__list__item__link">Machine Learning</a>
					</li>
					
					<li class="menu__list__item">
						<a href="/mlops/" class="menu__list__item__link">MLOps</a>
					</li>
					
				</ul>
			</div>
		</div>

	</div>

</header>


	<div class="loader"><svg width="120" height="30" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite" /></circle><circle cx="60" cy="15" r="9" fill-opacity="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite" /></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite" /></circle></svg></div>

	<div class="page-loader"></div>

	
	<div class="page">

		<div class="page__content" data-page-title="Singular Value Decomposition (SVD) ‚Äì ML with Ramin">

			<section class="hero hero--single">

	<div class="hero__image" style="background-image: url(/images/demo/demo-square.webp)">
		<div class="hero__overlay"></div>
	</div>

	<div class="wrap">

		<h1>Singular Value Decomposition (SVD)</h1>
		<!-- <p>02 January 2022</p> -->
		<p align="right"><b>- Yanming Liu<b></p>
			
	</div>

</section>

<section class="single">

	<div class="wrap">

		<article class="single-post">

			<h2 id="table-of-contents">Table of contents</h2>
<ol>
  <li>Significance</li>
  <li>Eigendecomposition</li>
  <li>SVD</li>
  <li>Geometric Meaning</li>
  <li>Summary</li>
</ol>

<h2 id="1-significance">1. Significance</h2>

<p><strong>SVD plays an important role in image processing like <a href="https://www.zhihu.com/question/22237507/answer/53804902">data compression and noise removal</a>.</strong></p>

<p>A gray image can be regarded as a matrix, $A$, of which the element is a pixel value. Suppose it is not a square matrix with a size of $800*600$. Matrix $A$ is a sum of items for each of them is a multiplication of a singular value, $œÉ_i$, and a small matrix $s_i$ the rank is $1$. In other words, $A = œÉ_1s_1 + œÉ_2s_2 + \ldots + œÉ_is_i + \sigma_ns_n = \sum_{i=1}^{n}\sigma_is_i$.</p>

<p>The singular values in SVD contain important information for the whole matrix. Some of them are overweight than other values, accompanied by a larger value. If we rank those values in descending order, pick the first $k$ values and their corresponding matrix, and calculate the sum of the multiplication of those pairs, $A‚Äô$ to approximate the whole matrix $A$. That is $A‚Äô=‚àë_{i=1}^{k}œÉ_is_i ‚âÉ A$, here we assume that ${\sigma_1, \ldots, \sigma_k}$ are exactly the largest values among all ${\sigma_1, \ldots, \sigma_n}$. In this way, we achieve data compression. For removing noise, it is similar. Noises were caused by items with small singular values, by ignoring those items, we can get data with better quality.</p>

<p>Now, we know the significance of SVD in an application. Before introducing the definition of SVD, we would better review its ‚Äúharmless brother‚Äù, Eigendecomposition.</p>

<h2 id="2-eigendecomposition">2. Eigendecomposition</h2>

<p><ins>Given $B$ with a size of $p*p$, the purpose of <a href="https://www.youtube.com/watch?v=PFDu9oVAE-g&amp;t=629s">eigendecompostion</a> is to find eigen-values $Œõ$ and eigen-vectors $ùêï$, let $BV = VŒõ$.Furthermore, $B=VŒõV^{-1}$</ins>, in which each column of $V$ is a eigen-vector $v_i$, and the diagonal value of $Œõ$ is a eigen-value $\lambda_i$, and $Œª_1 &gt; \lambda_2 &gt; \ldots &gt; Œª_p$.</p>

<p>The whole process can be seen in follow,</p>

\[BV = B(v_1, v_2, \ldots, v_p) 

= (\lambda_1v_1 + \lambda_2v_2 + \ldots + \lambda_pv_p) = (v_1, v_2, \ldots, v_p)\begin{bmatrix}Œª_1 &amp; \ldots &amp; 0\\
\vdots &amp; \ddots &amp; \vdots\\
0 &amp; \ldots &amp; Œª_p
\end{bmatrix} = VŒõ\]

<p>In summary, $BV = V\Lambda ‚áí B = VŒõV^{-1}$</p>

<p><ins>Let us consider a unique condition, $B$ is a symmetric matrix. The formula above can be written as $B=SŒõS^{T}$, for $S^{-1}=S^{T}$, and <a href="https://math.stackexchange.com/questions/537217/proof-of-orthogonal-matrix-property-a-1-at">$S$ is an orthogonal matrix</a></ins>. An orthogonal matrix means that every two rows or columns of that matrix would be orthogonal, and you will get an inner dot $0$ when multiplying these two row or column vectors. Additionally, each vector is a unit vector, $ |v_i|= 1 $.</p>

<p>$S$ can be represented as a horizontal stack of column vector $s_i$, $S = (s_1, s_2, \ldots, s_p)$.</p>

<p>Then, $B = Œª_1s_1s_1^{T} + Œª_2s_2s_2^{T} + \ldots + Œª_ps_ps_p^{T}$.</p>

<p><ins>Here is an example for decomposition of symmetric matrix.</ins></p>

<p>Given 
\(C=
\begin{bmatrix} 1 &amp; 2 \\
2 &amp; 1
\end{bmatrix}\).</p>

<p>Use \(\|C - ŒªI\| = 0\) to get all $Œª$s.</p>

<p>In detail, \(\left |\begin{array}{}
1-\lambda &amp; 2 \\
2 &amp; 1-\lambda
\end{array}\right| = 0 ‚áí \lambda_1=3, \lambda_2=-1\).</p>

<p>Eigen vectors, $s_1 = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})^{T}$, $s_2 = (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}})^{T}$, respectively.</p>

<p>\(B=\lambda_1s_1s_1^{T} + \lambda_2s_2s_2^{T} = 
\begin{bmatrix} 1 &amp; 2 \\
2 &amp; 1
\end{bmatrix}\).</p>

<p>This is the whole process for the decomposition of a symmetric matrix.</p>

<h2 id="3-svd">3. SVD</h2>
<p><ins>If the matrix is not a symmetric one, such as a $m*n$ matrix $D$, can we diagonalize the matrix? </ins> The answer is not, since the <a href="https://math.stackexchange.com/questions/441685/why-are-nonsquare-matrices-not-invertible">non-square matrix is not reversible</a>, if you would like to get $D = VŒõV^{-1}$. Needless to say the symmetric diagonalization. How can we extend such decomposition to a non-symmetric matrix? This introduces the Singular Value Decomposition (SVD).</p>

<p><ins>For SVD of matrix $D$, we aim to find $D = NŒ£U^{T}$. Here, $N = DD^{T}, U^{T}=D^{T}D $ with a size of $ m*m $ and $n * n$, respectively</ins>. $U$ and $N$ can also be represented as column vector form like $U= (u_1, u_2, \ldots, u_n), N= (n_1, n_2, \ldots, n_m) $. Because both are square and symmetric matric, we have $DD^{T} = NŒõ_1N^{T}, D^{T}D = UŒõ_2U^{T}$ as symmetric square matrix diagonalization suggests so. $\Sigma$, with a size of $m*n$, consists of all singular values, $œÉ_1, \sigma_2, \ldots, \sigma_{min(m,n)}$, in a descending order at the diagonal line.</p>

<p><ins>Let us conduct the SVD in an opposite way and calculate using an example</ins>. For obtaining $N, Œ£, U$, in the SVD formula, $D = NŒ£U^{T}$. We decompose the $DD^{T}$ to get $N$ and decompose $D^{T}D$ to obtain $U$. During the way, we also have $Œõ_1=Œõ_2$, of which the value in the diagonal line is $Œª_i$. For $Œ£$, the element $\sigma_i = \sqrt{Œª_i}$ (Ex. 1). Now, we are all set.</p>

<p>Given
\(D = \begin{bmatrix} 1 &amp; 2 \\
0 &amp; 0 \\
0 &amp; 0
\end{bmatrix}\)
,
\(DD^{T} = \begin{bmatrix} 5 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0
\end{bmatrix}\)</p>

<p>Decompose this part,</p>
<ul>
  <li>eigenvalues would be $\lambda_1=5, Œª_2=0, Œª_3=0$</li>
  <li>eigenvectors are $n_1=(1,0,0)^{T}, n_2=(0,1,0)^T, n_3=(0,0,1)^T$</li>
</ul>

<p>Similarly, for \(D^{T}D=\)
\(\begin{bmatrix} 1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}\)
,</p>
<ul>
  <li>$Œª_1=5, Œª_2=0$</li>
  <li>$u_1=(\frac{\sqrt{5}}{5}, \frac{2\sqrt{5}}{5})^T, u_2=(-\frac{2\sqrt{5}}{5}, \frac{\sqrt{5}}{5})^T$.</li>
</ul>

<p>Take \(Œ£ =\)
\(\begin{bmatrix}\sqrt{5} &amp; 0 \\
0 &amp; 0 \\
0 &amp; 0
\end{bmatrix}\)</p>

<p>Then SVD for $D$ is that $D=NŒ£Q^T=(n_1, n_2, n_3)Œ£(u_1, u_2)^T =$
\(\begin{bmatrix} 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0
\end{bmatrix}\begin{bmatrix} \sqrt{5} &amp; 0 \\
0 &amp; 0\\
0 &amp; 0
\end{bmatrix}\begin{bmatrix} \frac{\sqrt{5}}{5} &amp; \frac{2\sqrt{5}}{5} \\
\frac{2\sqrt{5}}{5} &amp; \frac{\sqrt{5}}{5}
\end{bmatrix} = \begin{bmatrix} 1 &amp; 2 \\
0 &amp; 0 \\
0 &amp; 0
\end{bmatrix}\).</p>

<p>Try to use the SVD process to solve for the example mentioned in section 2 to see if you can get the same result (Ex. 2).</p>

<h2 id="4-geometric-meaning">4. Geometric Meaning</h2>

<p><a href="https://www.youtube.com/watch?v=kYB8IZa5AuE">Matrix represents a spatial transformation</a>. <strong>SVD decomposes a complex transformation into three basic ones <a href="https://www.zhihu.com/question/20507061/answer/120540926">(rotate, scale, project)</a>. The size of the singular value marks the scaling effect of the corresponding singular vectors, which matters in the generation of final space.</strong> Thus, we use bigger singular values and vectors to construct the original data. For those interested in the intuition of SVD, I strongly recommend you read the <a href="https://www.zhihu.com/question/22237507/answer/53804902">answer with the second-highest votes</a> in reference.</p>

<p>To summarize what I learned in this reference blog, the SVD formula, $M=UŒ£V^{T}$ is introduced in a symmetric way. Transformation $M$ can be decoupled into three transformations, $V^{T}, Œ£, U$. $V^{T}$ is used to undo rotation operation, rotating one pair of orthogonal vectors, $v_1, v_2$, to horizontal and vertical direction, $e_1, e_2$. $Œ£$ is for the scaling those vectors to $\sigma_1e_1, \sigma_2e_2$. $U$ is another rotation to rotate those scaled vectors to the final position, $\sigma_1u_1, \sigma_2u_2$.</p>

<p align="center">
<a href="https://www.zhihu.com/question/22237507/answer/53804902">
<img src="/images/Posts/SVD/svd_geo_steps.webp" /></a>
Figure.<a href="https://www.zhihu.com/question/22237507/answer/53804902"> 
Matrix Decompostion Into Three Operations - Rotation, Scale, Rotation
 </a>
</p>

<h2 id="5-summary">5. Summary</h2>

<p>In this blog, we give a glimpse of the application of SVD in the industry such 
as data compression and noise removal. Next, we introduce the definition of
eigendecomposition on a square matrix and give an example of applying the operation
on a square &amp; symmetric matrix. Then, a question like if the same process can be 
on a non-square matrix is raised, and a solution like SVD is displayed 
with another example. Finally, we try to understand those math transformations
in geometry perspective.</p>

<p>For those tutorials written in Chinese, I suggest that you translate the 
pages for better understanding.</p>

<p>See further below,</p>

<p><a href="https://www.youtube.com/watch?v=CpD9XlTu3ys">SVD - Intro - Video</a>Ôºå <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Wiki - SVD</a>Ôºå</p>

<p><a href="https://www.zhihu.com/question/22237507/answer/53804902">SVD Application &amp; Geometry Meaning</a>Ôºå<a href="https://zhuanlan.zhihu.com/p/26306568?utm_id=0">SVD Decompostion - Process</a><br /></p>


		</article>

	</div>

</section>

		</div>

	</div>


	<footer class="footer">

	<div class="wrap">

		<p class="footer__text"></p>

		<div class="footer__copyright">
			<span>¬© 2023 ML with Ramin</span>
			<a href="https://jekyllthemes.io" target="_blank">Jekyll Themes</a>
		</div>

		<ul class="socials">
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	<li class="socials__item">
		<a href="https://medium.com/@mohammadi.r" target="_blank" class="socials__item__link" title="Medium">
			<i class="fab fa-medium" aria-hidden="true"></i>
		</a>
	</li>
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	<li class="socials__item">
		<a href="https://www.linkedin.com/in/ramin-mohammadi-ml/" target="_blank" class="socials__item__link" title="Linkedin">
			<i class="fab fa-linkedin" aria-hidden="true"></i>
		</a>
	</li>
	
	
</ul>

	</div>

</footer>


	<!-- Javascript Assets -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="/js/plugins-min.js"></script>
	<script src="/js/personal-min.js"></script>

	
	<!-- Extra Footer JS Code -->
	
	


</body>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


</html>