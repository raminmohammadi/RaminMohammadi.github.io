<!doctype html>

<html class="no-js" lang="en">

<head>


	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

	ML with Ramin

	Personal Theme by https://jekyllthemes.io
	Premium + free Jekyll themes for your blog or website.

	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->


	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

	<!-- Page Info -->
	<link rel="shortcut icon" href="/images/favicon.png">
	<title>All Sorts of Text Vectorization Techniques – ML with Ramin</title>
	<meta name="description" content="">

	<!-- Twitter Card -->
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="All Sorts of Text Vectorization Techniques – ML with Ramin">
	<meta name="twitter:description" content="">
	<meta name="twitter:image:src" content="http://localhost:4000/images/demo/demo-square.jpg">

	<!-- Facebook OpenGraph -->
	<meta property="og:title" content="All Sorts of Text Vectorization Techniques – ML with Ramin" />
	<meta property="og:description" content="" />
	<meta property="og:image" content="http://localhost:4000/images/demo/demo-square.jpg" />

	
	<!-- Font Embed Code -->
	<link href="https://fonts.googleapis.com/css?family=Muli:300,400,600,700" rel="stylesheet">
	

	<!-- Styles -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="/css/style.css">
	
	<!-- Icons -->
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/solid.js" integrity="sha384-GXi56ipjsBwAe6v5X4xSrVNXGOmpdJYZEEh/0/GqJ3JTHsfDsF8v0YQvZCJYAiGu" crossorigin="anonymous"></script>
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/brands.js" integrity="sha384-0inRy4HkP0hJ038ZyfQ4vLl+F4POKbqnaUB6ewmU4dWP0ki8Q27A0VFiVRIpscvL" crossorigin="anonymous"></script>
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/fontawesome.js" integrity="sha384-NY6PHjYLP2f+gL3uaVfqUZImmw71ArL9+Roi9o+I4+RBqArA2CfW1sJ1wkABFfPe" crossorigin="anonymous"></script>

	
	<!-- Custom Styles -->
	<style></style>
	

	
	<!-- Analytics Code -->
	
	

	
	<!-- Extra Header JS Code -->
	
	
	
</head>


<body class="loading ajax-loading" data-site-url="http://localhost:4000" data-page-url="/blog/vectorization">


	<header class="header">

	<div class="wrap">

		
		<a href="/" class="header__title">
			ML with Ramin
		</a>
		

		<div class="menu">
			<div class="menu__toggle js-menu-toggle">
				<div class="menu__toggle__icon"><span></span></div>
			</div>
			<div class="menu__wrap">
				<ul class="menu__list">
					
					<li class="menu__list__item">
						<a href="/about/" class="menu__list__item__link">About us</a>
					</li>
					
					<li class="menu__list__item">
						<a href="/ml/" class="menu__list__item__link">Machine Learning</a>
					</li>
					
					<li class="menu__list__item">
						<a href="/mlops/" class="menu__list__item__link">MLOps</a>
					</li>
					
				</ul>
			</div>
		</div>

	</div>

</header>


	<div class="loader"><svg width="120" height="30" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite" /></circle><circle cx="60" cy="15" r="9" fill-opacity="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite" /></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite" /></circle></svg></div>

	<div class="page-loader"></div>

	
	<div class="page">

		<div class="page__content" data-page-title="All Sorts of Text Vectorization Techniques – ML with Ramin">

			<section class="hero hero--single">

	<div class="hero__image" style="background-image: url(/images/demo/demo-square.jpg)">
		<div class="hero__overlay"></div>
	</div>

	<div class="wrap">

		<h1>All Sorts of Text Vectorization Techniques</h1>
		<!-- <p>20 March 2022</p> -->
		<p align="right"><b>- Ramin Mohammadi<b></p>
			
	</div>

</section>

<section class="single">

	<div class="wrap">

		<article class="single-post">

			<p>Text vectorization is the process of converting text into numerical vectors that can be processed by machine learning algorithms. There are various techniques for text vectorization, each with its own strengths and weaknesses. In this blog post, we will discuss four common text vectorization techniques and provide examples of each.</p>

<h3 id="1-bag-of-words">1. Bag of Words</h3>

<p>The bag of words model is a simple and popular technique for text vectorization. It involves creating a vocabulary of all unique words in the corpus and representing each document as a vector of word frequencies. The frequency of each word in the document is counted and assigned to the corresponding dimension in the vector.</p>

<p>Suppose we have the following three documents:</p>

<p>Document 1: “the cat in the hat” <br />
Document 2: “the cat sat on the mat” <br />
Document 3: “the dog chased the ball”</p>

<p>The vocabulary for these documents is [“ball”, “cat”, “chased”, “dog”, “hat”, “in”, “mat”, “on”, “sat”, “the”] in  alphabetical order. The vector representation for Document 1 is [0 1 0 0 1 1 0 0 0 2], the vector representation for Document 2 is [0 1 0 0 0 0 1 1 1 2], and the vector representation for Document 3 is [1 0 1 1 0 0 0 0 0 2].</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">the cat in the hat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">the cat sat on the mat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">the dog chased the ball</span><span class="sh">"</span><span class="p">]</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nf">toarray</span><span class="p">())</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0 1 0 0 1 1 0 0 0 2]
 [0 1 0 0 0 0 1 1 1 2]
 [1 0 1 1 0 0 0 0 0 2]]
</code></pre></div></div>

<h3 id="2-tf-idf">2. TF-IDF</h3>

<p>The <code class="language-plaintext highlighter-rouge">TF-IDF</code> (Term Frequency-Inverse Document Frequency) model is a more advanced technique for text vectorization that takes into account the importance of each word in the corpus. The TF-IDF value for a word in a document is calculated as the product of its term frequency and inverse document frequency.</p>

<p><code class="language-plaintext highlighter-rouge">TF</code> (Term Frequency) is calculated as the frequency of a word in a document divided by the total number of words in the document or just the raw count of a word in a document (we will use the later definition to explain below). <code class="language-plaintext highlighter-rouge">IDF</code> (Inverse Document Frequency) is calculated as the logarithm of the total number of documents, $N$, in the corpus divided by the number of documents that contain the word, $c$, i.e.,</p>

<p>\begin{equation}
    idf = log\frac{N}{c}
\end{equation}</p>

<p>For example, consider the same two documents from the bag of words example. The <code class="language-plaintext highlighter-rouge">TF-IDF</code> value for the word “the” in Document 1 is $1 * log(2/2) = 0.0$, and the <code class="language-plaintext highlighter-rouge">TF-IDF</code> value for the word “dog” in Document 2 is $1 * log(2/1) = 0.693$.</p>

<p>For smothing purpose, idf can be calculated using the formula below,</p>

<p>\begin{equation}
    idf = log\frac{N+1}{c+1} + 1
\end{equation}</p>

<p>For example, the <code class="language-plaintext highlighter-rouge">TF-IDF</code> value for the word “the” in Document 1 is $2 * (log((1+3)/(3+1))+1) = 2$, which 
can be seen as the last element of the first list inside the below Output.
The vocabulary for these documents is [“the”, “cat”, “in”, “hat”, “sat”, “on”, “mat”, “dog”, “chased”, “ball”]. The vector representation for Document 1 is [2, 1, 1, 1, 0, 0, 0, 0, 0, 0], the vector representation for Document 2 is [2, 1, 0, 0, 1, 1, 1, 0, 0, 0], and the vector representation for Document 3 is [2, 0, 0, 0, 0, 0, 1, 1, 1].</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">the cat in the hat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">the cat sat on the mat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">the dog chased the ball</span><span class="sh">"</span><span class="p">]</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">TfidfVectorizer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">vectorizer</span><span class="p">.</span><span class="n">vocabulary_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nf">toarray</span><span class="p">())</span>
</code></pre></div></div>

<p>Output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'the': 9, 'cat': 1, 'in': 5, 'hat': 4, 'sat': 8, 'on': 7, 'mat': 6, 'dog': 3, 'chased': 2, 'ball': 0}
[[0.         1.28768207 0.         0.         1.69314718 1.69314718
  0.         0.         0.         2.        ]
 [0.         1.28768207 0.         0.         0.         0.
  1.69314718 1.69314718 1.69314718 2.        ]
 [1.69314718 0.         1.69314718 1.69314718 0.         0.
  0.         0.         0.         2.        ]]
</code></pre></div></div>

<p>As most model will require a normalized input matrix, l2 normalization is recommended for the above output.
What we need to do is to assign the parameter <code class="language-plaintext highlighter-rouge">norm='l2'</code> or leave it as an empty (default),</p>

<h4 id="tf-idf">TF-IDF</h4>
<p>Suppose we have the same three documents as in the previous example. The term frequency for the word “cat” in Document 1 is 1/5, the term frequency for “cat” in Document 2 is 1/6, and the term frequency for “cat” in Document 3 is 0/5. The inverse document frequency for “cat” is log(3/2) = 0.176, since there are three documents in the corpus and “cat” appears in two of them. The TF-IDF value for “cat” in Document 1 is (1/5) * 0.176 = 0.035, the TF-IDF value for “cat” in Document 2 is (1/6) * 0.176 = 0.029, and the TF-IDF value for “cat” in Document 3 is 0 * 0.176 = 0.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">TfidfVectorizer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>In other words,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">toarray</span><span class="p">()</span>
<span class="n">X</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0.         0.38151877 0.         0.         0.50165133 0.50165133
  0.         0.         0.         0.59256672]
 [0.         0.34101521 0.         0.         0.         0.
  0.44839402 0.44839402 0.44839402 0.52965746]
 [0.4769856  0.         0.4769856  0.4769856  0.         0.
  0.         0.         0.         0.56343076]]
</code></pre></div></div>

<h3 id="3-word-embeddings">3. Word Embeddings</h3>

<p>Word embeddings are a type of text vectorization technique that maps each word in the corpus to a low-dimensional vector. The vectors are learned through a neural network that takes into account the context in which each word appears. Word embeddings capture the semantic and syntactic meaning of words and are useful for various natural language processing tasks.</p>

<p>To develop an embedding using TensorFlow and Keras, follow these steps:</p>

<ul>
  <li>Import necessary libraries</li>
  <li>Prepare your data (tokenization and padding)</li>
  <li>Create a model with an Embedding layer</li>
  <li>Compile and train the model</li>
</ul>

<p>Here’s a simple example to demonstrate these steps:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Import necessary libraries
</span><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="c1"># Step 2: Prepare your data (tokenization and padding)
</span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">I love machine learning</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Natural language processing is fascinating</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Deep learning is a subset of machine learning</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="sh">"</span><span class="s">&lt;OOV&gt;</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<span class="n">padded_sequences</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">post</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Step 3: Create a model with an Embedding layer
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>  <span class="c1"># You can use other layers such as GlobalAveragePooling1D as well
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Step 4: Compile and train the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">binary_crossentropy</span><span class="sh">"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="sh">"</span><span class="s">adam</span><span class="sh">"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># Dummy labels for demonstration purposes
</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Train the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">padded_sequences</span><span class="p">.</span><span class="nf">tolist</span><span class="p">(),</span> <span class="n">labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

</code></pre></div></div>

<p>This example demonstrates how to create a simple model with an Embedding layer using TensorFlow and Keras. First, the sentences are tokenized and converted to padded sequences. Then, a model is created with an Embedding layer followed by a Flatten layer, a dense layer with 6 units and ReLU activation, and an output layer with a single unit and sigmoid activation. Finally, the model is compiled and trained on the padded sequences and dummy labels.</p>

<p>After training the model, you can access the trained embedding layer’s weights to inspect or use them for other purposes. Here’s how you can do that:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get the embedding layer
</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Access the weights of the embedding layer
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">.</span><span class="nf">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># The shape of the weights matrix will be (input_dim, output_dim)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of the weights matrix:</span><span class="sh">"</span><span class="p">,</span> <span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Get the embedding for a specific word
</span><span class="n">word_index_to_find</span> <span class="o">=</span> <span class="n">word_index</span><span class="p">[</span><span class="sh">"</span><span class="s">machine</span><span class="sh">"</span><span class="p">]</span>
<span class="n">word_embedding</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">word_index_to_find</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Embedding for </span><span class="sh">'</span><span class="s">machine</span><span class="sh">'</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">word_embedding</span><span class="p">)</span>
</code></pre></div></div>

<p>In this example, we first get the embedding layer from the model using <code class="language-plaintext highlighter-rouge">model.layers[0]</code>, as the embedding layer is the first layer in our model. Then, we access the weights of the embedding layer using <code class="language-plaintext highlighter-rouge">embedding_layer.get_weights()[0]</code>. The shape of the weights matrix will be <code class="language-plaintext highlighter-rouge">(input_dim, output_dim)</code>.</p>

<p>To get the embedding for a specific word, we can look up the word index using the tokenizer’s word index dictionary, and then access the corresponding row in the weights matrix. In this example, we find the embedding for the word “machine”.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Shape of the weights matrix: (100, 16)
Embedding for 'machine': [ 0.02413699 -0.05747801 -0.05382366 -0.00870479  0.04794465 -0.0100428
 -0.00971722 -0.03319969  0.04297956  0.04293872 -0.0194245  -0.03947173
  0.02845843  0.00695275  0.03985218 -0.01657288]
</code></pre></div></div>

<h3 id="4-doc2vec">4. Doc2Vec</h3>

<p>Doc2Vec is a text vectorization technique that extends the idea of word embeddings to entire documents. It represents each document as a low-dimensional vector that captures its meaning and context. Doc2Vec is trained using a neural network that takes into account both the words in the document and the document’s unique identifier.</p>

<p>For example, consider a corpus of news articles. The vector representation for an article might be [0.3, -0.2, 0.1, 0.-0.5], where each dimension represents a different aspect of the article’s content or sentiment.</p>

<p>Below is an example of using Doc2Vec with TensorFlow and Keras. In this example, we’ll use the IMDB dataset to create document embeddings and classify movie reviews as positive or negative. Please note that Doc2Vec is not natively supported by TensorFlow or Keras, but we can achieve a similar effect using a custom approach.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">imdb</span>

<span class="c1"># Load the IMDB dataset
</span><span class="n">max_features</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">maxlen</span> <span class="o">=</span> <span class="mi">300</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">.</span><span class="nf">load_data</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="n">sequence</span><span class="p">.</span><span class="nf">pad_sequences</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="n">sequence</span><span class="p">.</span><span class="nf">pad_sequences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Doc2Vec</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">max_features</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Doc2Vec</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">max_features</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">GlobalAveragePooling1D</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">Doc2Vec</span><span class="p">(</span><span class="n">max_features</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Train the model
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>

<span class="c1"># Evaluate the model
</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Test accuracy: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Get the word index from the IMDB dataset
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">.</span><span class="nf">get_word_index</span><span class="p">()</span>

<span class="c1"># Define a sample document
</span><span class="n">sample_doc</span> <span class="o">=</span> <span class="sh">"</span><span class="s">This movie is fantastic! I really enjoyed it and highly recommend it.</span><span class="sh">"</span>

<span class="c1"># Preprocess the sample document
</span><span class="k">def</span> <span class="nf">preprocess_doc</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">word_index</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">300</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="nf">lower</span><span class="p">().</span><span class="nf">split</span><span class="p">()</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_index</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>  <span class="c1"># 1 is the index for out-of-vocabulary words
</span>    <span class="n">padded_tokens</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="n">sequence</span><span class="p">.</span><span class="nf">pad_sequences</span><span class="p">([</span><span class="n">token_ids</span><span class="p">],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padded_tokens</span>

<span class="n">sample_doc_preprocessed</span> <span class="o">=</span> <span class="nf">preprocess_doc</span><span class="p">(</span><span class="n">sample_doc</span><span class="p">,</span> <span class="n">word_index</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span>

<span class="c1"># Obtain the embeddings for the sample document
</span><span class="n">doc_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_embeddings</span><span class="p">(</span><span class="n">sample_doc_preprocessed</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Document embeddings shape: </span><span class="si">{</span><span class="n">doc_embeddings</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Document embeddings: </span><span class="si">{</span><span class="n">doc_embeddings</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Part of output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Document embeddings shape: (1, 128)
Document embeddings: [[-0.03332498  0.02149555 -0.03570618 -0.01496935 -0.02809953  0.02970941
   0.02491445 -0.02052332  0.02021462 -0.00954685  0.03282563  0.01585981
  -0.03508868 -0.03854481  0.04335667  0.00857451  0.03536903  0.02139922
   0.02785825 -0.01258065  0.02099344 -0.04324191 -0.02876599  0.0381836
  -0.04068084  0.01699455  0.04381409 -0.00950469  0.02788743  0.04080617
  -0.01029504 -0.04217651 -0.0207891  -0.02794119 -0.04348803  0.03267356
   0.0122643   0.01809947 -0.02824404  0.0200795   0.04166844  0.00185017
   0.04230817 -0.02399788  0.0047141   0.01540776  0.00130954  0.01412019
   0.01188601  0.03322793  0.02316885 -0.02906333 -0.00211944  0.00920796
   0.03495089 -0.00085822 -0.03712093 -0.03742212  0.01604812 -0.0028497
  -0.01565623  0.00659219 -0.01928242 -0.01757747  0.0259659  -0.02001496
   0.01374836  0.02165808 -0.02058002  0.01073473 -0.03319663 -0.04416433
  -0.00037649 -0.02048877 -0.02593763 -0.02201134  0.02046788  0.03230011
   0.01069888  0.02703679  0.04115723 -0.00040947  0.00611654  0.0161706
  -0.00828066  0.01221789  0.03822241 -0.02687576  0.0035094  -0.02249913
   0.00462279  0.02576657  0.02639207  0.03381234 -0.0120204   0.0217866
  -0.0303988  -0.00331894  0.01864516  0.02433652 -0.02785279  0.00559374
   0.009917   -0.00494993 -0.04006478  0.00984478 -0.00093492 -0.03563965
  -0.02665879 -0.01499166 -0.00409812  0.02898056 -0.00355999 -0.04162361
   0.04168115  0.01700404 -0.02436942  0.02686478  0.01744992  0.02253166
   0.04268411 -0.00433293  0.0436205   0.02046859 -0.0444436  -0.01251493
  -0.04224044 -0.00371554]]
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>Text vectorization is a crucial step in many natural language processing tasks. In this blog post, we have discussed four common text vectorization techniques: bag of words, TF-IDF, word embeddings, and Doc2Vec. Each technique has its own strengths and weaknesses, and the choice of technique depends on the specific task and the characteristics of the corpus. By understanding these techniques and their mathematical formulas, we can better preprocess text data and improve the performance of our machine learning models.</p>


		</article>

	</div>

</section>

		</div>

	</div>


	<footer class="footer">

	<div class="wrap">

		<p class="footer__text"></p>

		<div class="footer__copyright">
			<span>© 2023 ML with Ramin</span>
			<a href="https://jekyllthemes.io" target="_blank">Jekyll Themes</a>
		</div>

		<ul class="socials">
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	<li class="socials__item">
		<a href="https://medium.com/@mohammadi.r" target="_blank" class="socials__item__link" title="Medium">
			<i class="fab fa-medium" aria-hidden="true"></i>
		</a>
	</li>
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	<li class="socials__item">
		<a href="https://www.linkedin.com/in/ramin-mohammadi-ml/" target="_blank" class="socials__item__link" title="Linkedin">
			<i class="fab fa-linkedin" aria-hidden="true"></i>
		</a>
	</li>
	
	
</ul>

	</div>

</footer>


	<!-- Javascript Assets -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="/js/plugins-min.js"></script>
	<script src="/js/personal-min.js"></script>

	
	<!-- Extra Footer JS Code -->
	
	


</body>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


</html>