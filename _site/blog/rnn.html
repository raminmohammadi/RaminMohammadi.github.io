<!doctype html>

<html class="no-js" lang="en">

<head>


	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

	ML with Ramin

	Personal Theme by https://jekyllthemes.io
	Premium + free Jekyll themes for your blog or website.

	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->


	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

	<!-- Page Info -->
	<link rel="shortcut icon" href="/images/favicon.png">
	<title>Recurrent Neural Networks - A Beginner's Guide – ML with Ramin</title>
	<meta name="description" content="">

	<!-- Twitter Card -->
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="Recurrent Neural Networks - A Beginner's Guide – ML with Ramin">
	<meta name="twitter:description" content="">
	<meta name="twitter:image:src" content="http://localhost:4000/images/demo/demo-square.jpg">

	<!-- Facebook OpenGraph -->
	<meta property="og:title" content="Recurrent Neural Networks - A Beginner's Guide – ML with Ramin" />
	<meta property="og:description" content="" />
	<meta property="og:image" content="http://localhost:4000/images/demo/demo-square.jpg" />

	
	<!-- Font Embed Code -->
	<link href="https://fonts.googleapis.com/css?family=Muli:300,400,600,700" rel="stylesheet">
	

	<!-- Styles -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="/css/style.css">
	
	<!-- Icons -->
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/solid.js" integrity="sha384-GXi56ipjsBwAe6v5X4xSrVNXGOmpdJYZEEh/0/GqJ3JTHsfDsF8v0YQvZCJYAiGu" crossorigin="anonymous"></script>
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/brands.js" integrity="sha384-0inRy4HkP0hJ038ZyfQ4vLl+F4POKbqnaUB6ewmU4dWP0ki8Q27A0VFiVRIpscvL" crossorigin="anonymous"></script>
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/fontawesome.js" integrity="sha384-NY6PHjYLP2f+gL3uaVfqUZImmw71ArL9+Roi9o+I4+RBqArA2CfW1sJ1wkABFfPe" crossorigin="anonymous"></script>

	
	<!-- Custom Styles -->
	<style></style>
	

	
	<!-- Analytics Code -->
	
	

	
	<!-- Extra Header JS Code -->
	
	
	
</head>


<body class="loading ajax-loading" data-site-url="http://localhost:4000" data-page-url="/blog/rnn">


	<header class="header">

	<div class="wrap">

		
		<a href="/" class="header__title">
			ML with Ramin
		</a>
		

		<div class="menu">
			<div class="menu__toggle js-menu-toggle">
				<div class="menu__toggle__icon"><span></span></div>
			</div>
			<div class="menu__wrap">
				<ul class="menu__list">
					
					<li class="menu__list__item">
						<a href="/about/" class="menu__list__item__link">About us</a>
					</li>
					
					<li class="menu__list__item">
						<a href="/ml/" class="menu__list__item__link">Machine Learning</a>
					</li>
					
					<li class="menu__list__item">
						<a href="/mlops/" class="menu__list__item__link">MLOps</a>
					</li>
					
				</ul>
			</div>
		</div>

	</div>

</header>


	<div class="loader"><svg width="120" height="30" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite" /></circle><circle cx="60" cy="15" r="9" fill-opacity="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite" /></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite" /></circle></svg></div>

	<div class="page-loader"></div>

	
	<div class="page">

		<div class="page__content" data-page-title="Recurrent Neural Networks - A Beginner's Guide – ML with Ramin">

			<section class="hero hero--single">

	<div class="hero__image" style="background-image: url(/images/demo/demo-square.jpg)">
		<div class="hero__overlay"></div>
	</div>

	<div class="wrap">

		<h1>Recurrent Neural Networks - A Beginner's Guide</h1>
		<!-- <p>24 March 2022</p> -->
		<p align="right"><b>- Ramin Mohammadi<b></p>
			
	</div>

</section>

<section class="single">

	<div class="wrap">

		<article class="single-post">

			<h3 id="introduction">Introduction</h3>
<p>Recurrent Neural Networks (RNNs) are a type of artificial neural network that are particularly useful in modeling sequential data. They are widely used in natural language processing, speech recognition, image captioning, and many other applications. In this article, we will provide an introduction to RNNs, explaining the basic concepts and mathematical expressions involved.</p>

<h3 id="background">Background</h3>
<p>Before diving into RNNs, let’s review some basics of neural networks. A neural network consists of layers of interconnected nodes, or neurons, that are organized into an input layer, one or more hidden layers, and an output layer. Each neuron takes in one or more inputs, computes a weighted sum of those inputs, and applies a non-linear activation function to the result.</p>

<p>The weights and biases of the neurons are learned through a process called backpropagation, in which the network is trained on a set of labeled examples to minimize the error between the predicted output and the true output. This process involves iteratively adjusting the weights and biases of the neurons to improve the network’s performance.</p>

<h3 id="rnns">RNNs</h3>
<p>RNNs are a type of neural network that are designed to handle sequential data, such as time series or natural language sentences. Unlike traditional feedforward neural networks, which process all the inputs at once and produce a single output, RNNs process the inputs one at a time, and maintain an internal state that depends on the previous inputs.</p>

<p>The basic structure of an RNN is shown in the below figure. At each time step, the network takes in an input $x_t$ and produces an output $y_t$, as well as updating its internal state $h_t$. The internal state is passed from one time step to the next, allowing the network to “remember” previous inputs and incorporate them into its output.</p>

<p align="center">
<img src="/images/Posts/NLP/rnn.jpg" style="display: block;
margin-left: auto;
margin-right: auto; height:350px;width:600px" />
</p>

<h4 id="the-recurrence-formula">The Recurrence Formula</h4>
<p>The internal state $h_t$ of an RNN is computed using a recurrence formula that depends on the current input $x_t$ and the previous internal state $h_{t-1}$. This formula is given by:</p>

<p>\begin{equation}
h_t = f(W_{hx} x_t + W_{hh} h_{t-1} + b_h)
\end{equation}</p>

<p>where $W_{hx}$ is the weight matrix that connects the input to the hidden state, $W_{hh}$ is the weight matrix that connects the hidden state to itself, $b_h$ is the bias term, and $f$ is the activation function. Note that the same weights $W_{hx}$ and $W_{hh}$ are used at every time step, which allows the network to learn a single set of parameters that can be applied to any input sequence.</p>

<h4 id="the-output-function">The Output Function</h4>
<p>The output $y_t$ of an RNN at time step $t$ is computed using another set of weights $W_{yh}$ that connect the internal state to the output:</p>

\[y_t = g(W_{yh} h_t + b_y)\]

<p>where $b_y$ is the bias term and $g$ is the activation function for the output. This function can be a simple linear function for regression tasks or a softmax function for classification tasks.</p>

<h4 id="backpropagation-through-time">Backpropagation Through Time</h4>
<p>Like other neural networks, RNNs are trained using backpropagation. However, since the recurrence formula involves the internal state at previous time steps, the gradient computation needs to be modified. This is known as Backpropagation Through Time (BPTT).</p>

<p>In BPTT, the error is backpropagated through the entire sequence, from the final output back to the initial state. The weight updates are then computed by summing the gradients at each time step. This approach can be computationally expensive for long sequences, and can suffer from the vanishing gradient problem, where the gradients become very small and the network stops learning.</p>

<h4 id="variations-of-rnns">Variations of RNNs</h4>
<p>There are several variations of RNNs that address the issues with BPTT and improve performance. Two commonly used variations are the Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks.</p>

<p>LSTMs use a memory cell that can be written to or read from, allowing the network to selectively retain or discard information from previous time steps. GRUs use gating mechanisms to selectively update the internal state and output. These variations have been shown to be effective in modeling long-term dependencies and avoiding the vanishing gradient problem.</p>

<h3 id="conclusion">Conclusion</h3>
<p>Recurrent Neural Networks are a powerful tool for modeling sequential data. By maintaining an internal state that depends on previous inputs, RNNs can “remember” and incorporate past information into their output. While training RNNs can be challenging due to the recurrence and long-term dependencies, variations such as LSTM and GRU networks have been shown to address these issues and achieve state-of-the-art performance in many tasks.</p>


		</article>

	</div>

</section>

		</div>

	</div>


	<footer class="footer">

	<div class="wrap">

		<p class="footer__text"></p>

		<div class="footer__copyright">
			<span>© 2023 ML with Ramin</span>
			<a href="https://jekyllthemes.io" target="_blank">Jekyll Themes</a>
		</div>

		<ul class="socials">
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	<li class="socials__item">
		<a href="https://medium.com/@mohammadi.r" target="_blank" class="socials__item__link" title="Medium">
			<i class="fab fa-medium" aria-hidden="true"></i>
		</a>
	</li>
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	<li class="socials__item">
		<a href="https://www.linkedin.com/in/ramin-mohammadi-ml/" target="_blank" class="socials__item__link" title="Linkedin">
			<i class="fab fa-linkedin" aria-hidden="true"></i>
		</a>
	</li>
	
	
</ul>

	</div>

</footer>


	<!-- Javascript Assets -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="/js/plugins-min.js"></script>
	<script src="/js/personal-min.js"></script>

	
	<!-- Extra Footer JS Code -->
	
	


</body>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


</html>